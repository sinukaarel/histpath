---
toc: false
layout: post
description: "Classify and detect cells in testis histopathology images"
categories: [histopathology]
title: "Cell detection and classification"
---

# Cell detection and classification

## Task

Physicians can identify men’s infertility by counting specific cells in testis histopathology images. Such annotation is hard manual labor that can be automated using neural networks. Ida-Tallinn hospital provided a large dataset of these images to build a cell detector and classifier to ease specialists’ lives.

## WSI and MIRAX format

Training images WSI (whole slide image) come in MIRAX format - a special kind of virtual slide format. Virtual slide is a high-resolution digital image of the microscopy sample that is created by sliding the glass past the camera and taking multiple pictures.

These created images are too high-resolution to practically handle as image tiles and are broken up into multiple files. This kind of processing needs special notation and a system in order to view and access data after storage.

On disk the format includes:

- `.mrxs` file that holds the **metadata** including information on how `.dat` files are organized
- data folder with `.dat` files that hold the actual **image data**
- `Index.dat` file in data folder that holds the **pointers** to locate the image data
- `.json` **annotation** file in JSON format that holds the metadata for the annotated section of the image
- `Slidedat.ini` file in data folder that contains structural data and **parameters of the scanning process** in `key=value` format. These values are also used in order to receive image data.

### Example of a single slide

This is a an example of a single training slide. The WSI image has 10 different levels with each level becoming less detailed as the level grows. In this case LVL 10 is the top most layer and the one that has the smallest dimensions and can be used only for thumbnail. The LVL 1 is the lowest layer and has highest dimensions and is most precise. Another factor to consider is that actually most of the image is empty, only proportion of it is actual tissue as can be seen from the LVL 10 image.

![]({{site.baseurl}}/images/histpath/example_slide.png "Example of a single slide structure")

### Training dataset

Training dataset consist of 4 WSI slides and each slide has 200 annotations. These annotated cells are different stages of spermatozoid life cycle and based of their proportions, one can diagnose infertility.

| Spermatozoid type    | Annotated cell count |
| -------------------- | -------------------- |
| Spermatogonia        | 181                  |
| Primary spermatocyte | 180                  |
| Spermatid            | 180                  |
| Sertoli              | 179                  |
| Spermatozoa          | 80                   |

## Downscaling images

As the whole-slide images are too big to process it is required to downscale the whole image into smaller tiles. This required to scan the slide with user selected tile size and then collect annotation data if any existed for that tile. Annotated cell center point was used to determine if the cell is on the tile or not. Tiles images were saved and a annotations for the images were saved in [COCO](https://cocodataset.org/#format-data) format for easier processing. COCO format is something that could be used as an input for different models and frameworks and makes model/framework changing a lot easier if required.

### FiftyOne dataset visualization

To get a better overview of the annotation correctness and insights of the data [FiftyOne](https://voxel51.com/) was used. This is a great solution for dataset and model predictions visualization and finding mistakes in annotated images. Good overview of training dataset and prediction visualization help to understand insights of trained model much better. Some of the training images had many annotations, but there were images where only a singe annotated cell was visible.

![]({{site.baseurl}}/images/histpath/many_vs_single_ann.png "FiftyOne dataset visualization")

## Training detection model

After solving the downscaling problem and generating a dataset that can be trained upon it is time to move on to selecting a machine learning framework and a model to train. There are multiple options here to go with, initially [Center-Net](https://github.com/FateScript/CenterNet-better), was the model in mind, but due to implementation difficulties [Pytorch and Faster R-CNN](https://pytorch.org/vision/stable/models.html) architecture was used.

### Custom data loader

In order to pass generated dataset to the Pytorch model a custom data loader needs to be implemented. Unfortunately there is a great [tutorial](https://medium.com/fullstackai/how-to-train-an-object-detector-with-your-own-coco-dataset-in-pytorch-319e7090da5) how to implement the data loader for a custom COCO dataset. Also the Pytorch has a good boilerplate for detection training scripts that can be found in their [github repo](https://github.com/pytorch/vision/tree/master/references/detection). Combining these two great sources allowed to write the training script more easily.

### Augmentations

Detection task needs a little more work than classification task when it comes to image augmentation as the original bounding boxes on images also need to shift and follow the augmented image corresponding sections, so that they also match the cell after augmentation. As the annotation was considered on the image if the annotation center point was on the selected tile, then the bounding boxes shape also need to be limited by the border of the image.

![]({{site.baseurl}}/images/histpath/limit_bboxes.png "Cutting excessive bounding boxes")

In order for the model to generalize and achieve better performance image augmentations were used in training process. [Albumentations](http://albumentations.ai/) library was used in order to distort the original images. The training images transforms consisted of [Flipping](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L288), [RandomBrightnessContrast](https://github.com/albumentations-team/albumentations/blob/a3f40853b33de19f8c64f5d977190a6cdd0c786c/albumentations/augmentations/transforms.py#L1635-L1677), [HueSaturationValue](https://github.com/albumentations-team/albumentations/blob/a3f40853b33de19f8c64f5d977190a6cdd0c786c/albumentations/augmentations/transforms.py#L1414-L1457) augmentations and [CLAHE](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L1993) or [FancyPCA](https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L2466) augmentations.

![]({{site.baseurl}}/images/histpath/augmentations.png "Augmentation of training images")

### Training

Model was trained on the Tartu University [HPC](https://hpc.ut.ee/en/home/) (High Performance Computing Center) using NVIDIA Tesla V100 GPU.

### Using images from different levels

After implementing the first model with only level 1 (the layer with largest dimensions) images it seemed that the model might improve by seeing cells from different height as human does when annotating different cell types. The level 1 image might provide higher details on how the exact cell looks and how to distinguish between different cells, but the higher level images might provide overview regarding cell positioning, detection etc.

![]({{site.baseurl}}/images/histpath/scaling_layers.png "Images from different levels")

Taking images from different levels meant also that the bounding boxes of level 1 annotations needed to be scaled accordingly. As each level were scaled to cover twice the dimension of previous level, then using images above level 3 seemed to be not effective as the annotations got smaller and smaller.

## Validation

### Eliminating predictions

Non-Max Suppression is a technique to filter the predictions of object detections. The idea of the Non-Max Suppression is to keep the highest scoring bounding box for a single detection. The IOU (intersection over union) of two bounding boxes is calculated to determine the boxes that overlap and lowest scoring boxes are dropped. This can reduce the predictions noise quite a lot as can be seen in the following image.

![]({{site.baseurl}}/images/histpath/supression_vs_none.png "Using non-max suppression")

```
TODO:
Mainly, I have to improve model performance.
Current issues:
- model detects cells very well but can't distinguish between different types
- The slides are not fully annotated and only some cells on the slides are.
  This creates a situation where cells are found, but as all of them are not annotated
  they are considered falsy. This allows model to find cells but not distinguish
  between different types.
```

## Conclusion

There is lot of work before training the neural network model. Data comes in all sorts of different formats and the preprocessing takes most of the time. Also the job isn't finished when you have reached the model training phase, there is always room for improvement. Otherwise, lots of new knowledge with WSI, Pytorch, Python, FastPages, using HPC Cluster etc.

Source Code available in:
[GitHub](https://github.com/sinukaarel/histpath-data)
![]({{site.baseurl}}/images/histpath/qrcode.png)

Some Good Resources:

- Downscaling whole-slide images in Python [link](https://developer.ibm.com/technologies/data-science/articles/an-automatic-method-to-identify-tissues-from-big-whole-slide-images-pt1/)
- Use tiling and generate smaller images where the sample occupying atleast 90% of the area [link](https://web.stanford.edu/group/rubinlab/pubs/2243353.pdf)
- Deephistopath [link](https://github.com/CODAIT/deep-histopath)
