<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Cell detection and classification | fastpages</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Cell detection and classification" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Classify and detect cells in testis histopathology images" />
<meta property="og:description" content="Classify and detect cells in testis histopathology images" />
<link rel="canonical" href="https://sinukaarel.github.io/histpath/histopathology/2020/01/21/histpath.html" />
<meta property="og:url" content="https://sinukaarel.github.io/histpath/histopathology/2020/01/21/histpath.html" />
<meta property="og:site_name" content="fastpages" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-01-21T00:00:00-06:00" />
<script type="application/ld+json">
{"url":"https://sinukaarel.github.io/histpath/histopathology/2020/01/21/histpath.html","@type":"BlogPosting","headline":"Cell detection and classification","dateModified":"2020-01-21T00:00:00-06:00","datePublished":"2020-01-21T00:00:00-06:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://sinukaarel.github.io/histpath/histopathology/2020/01/21/histpath.html"},"description":"Classify and detect cells in testis histopathology images","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/histpath/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://sinukaarel.github.io/histpath/feed.xml" title="fastpages" /><link rel="shortcut icon" type="image/x-icon" href="/histpath/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />

<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/histpath/">fastpages</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/histpath/about/">About Me</a><a class="page-link" href="/histpath/search/">Search</a><a class="page-link" href="/histpath/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Cell detection and classification</h1><p class="page-description">Classify and detect cells in testis histopathology images</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-01-21T00:00:00-06:00" itemprop="datePublished">
        Jan 21, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      6 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/histpath/categories/#histopathology">histopathology</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <h1 id="cell-detection-and-classification">Cell detection and classification</h1>

<h2 id="task">Task</h2>

<p>Physicians can identify men’s infertility by counting specific cells in testis histopathology images. Such annotation is hard manual labor that can be automated using neural networks. Ida-Tallinn hospital provided a large dataset of these images to build a cell detector and classifier to ease specialists’ lives.</p>

<h2 id="wsi-and-mirax-format">WSI and MIRAX format</h2>

<p>Training images WSI (whole slide image) come in MIRAX format - a special kind of virtual slide format. Virtual slide is a high-resolution digital image of the microscopy sample that is created by sliding the glass past the camera and taking multiple pictures.</p>

<p>These created images are too high-resolution to practically handle as image tiles and are broken up into multiple files. This kind of processing needs special notation and a system in order to view and access data after storage.</p>

<p>On disk the format includes:</p>

<ul>
  <li><code class="language-plaintext highlighter-rouge">.mrxs</code> file that holds the <strong>metadata</strong> including information on how <code class="language-plaintext highlighter-rouge">.dat</code> files are organized</li>
  <li>data folder with <code class="language-plaintext highlighter-rouge">.dat</code> files that hold the actual <strong>image data</strong></li>
  <li><code class="language-plaintext highlighter-rouge">Index.dat</code> file in data folder that holds the <strong>pointers</strong> to locate the image data</li>
  <li><code class="language-plaintext highlighter-rouge">.json</code> <strong>annotation</strong> file in JSON format that holds the metadata for the annotated section of the image</li>
  <li><code class="language-plaintext highlighter-rouge">Slidedat.ini</code> file in data folder that contains structural data and <strong>parameters of the scanning process</strong> in <code class="language-plaintext highlighter-rouge">key=value</code> format. These values are also used in order to receive image data.</li>
</ul>

<h3 id="example-of-a-single-slide">Example of a single slide</h3>

<p>This is a an example of a single training slide. The WSI image has 10 different levels with each level becoming less detailed as the level grows. In this case LVL 10 is the top most layer and the one that has the smallest dimensions and can be used only for thumbnail. The LVL 1 is the lowest layer and has highest dimensions and is most precise. Another factor to consider is that actually most of the image is empty, only proportion of it is actual tissue as can be seen from the LVL 10 image.</p>

<p><img src="/histpath/images/histpath/example_slide.png" alt="" title="Example of a single slide structure" /></p>

<h3 id="training-dataset">Training dataset</h3>

<p>Training dataset consist of 4 WSI slides and each slide has 200 annotations. These annotated cells are different stages of spermatozoid life cycle and based of their proportions, one can diagnose infertility.</p>

<table>
  <thead>
    <tr>
      <th>Spermatozoid type</th>
      <th>Annotated cell count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Spermatogonia</td>
      <td>181</td>
    </tr>
    <tr>
      <td>Primary spermatocyte</td>
      <td>180</td>
    </tr>
    <tr>
      <td>Spermatid</td>
      <td>180</td>
    </tr>
    <tr>
      <td>Sertoli</td>
      <td>179</td>
    </tr>
    <tr>
      <td>Spermatozoa</td>
      <td>80</td>
    </tr>
  </tbody>
</table>

<h2 id="downscaling-images">Downscaling images</h2>

<p>As the whole-slide images are too big to process it is required to downscale the whole image into smaller tiles. This required to scan the slide with user selected tile size and then collect annotation data if any existed for that tile. Annotated cell center point was used to determine if the cell is on the tile or not. Tiles images were saved and a annotations for the images were saved in <a href="https://cocodataset.org/#format-data">COCO</a> format for easier processing. COCO format is something that could be used as an input for different models and frameworks and makes model/framework changing a lot easier if required.</p>

<h3 id="fiftyone-dataset-visualization">FiftyOne dataset visualization</h3>

<p>To get a better overview of the annotation correctness and insights of the data <a href="https://voxel51.com/">FiftyOne</a> was used. This is a great solution for dataset and model predictions visualization and finding mistakes in annotated images. Good overview of training dataset and prediction visualization help to understand insights of trained model much better. Some of the training images had many annotations, but there were images where only a singe annotated cell was visible.</p>

<p><img src="/histpath/images/histpath/many_vs_single_ann.png" alt="" title="FiftyOne dataset visualization" /></p>

<h2 id="training-detection-model">Training detection model</h2>

<p>After solving the downscaling problem and generating a dataset that can be trained upon it is time to move on to selecting a machine learning framework and a model to train. There are multiple options here to go with, initially <a href="https://github.com/FateScript/CenterNet-better">Center-Net</a>, was the model in mind, but due to implementation difficulties <a href="https://pytorch.org/vision/stable/models.html">Pytorch and Faster R-CNN</a> architecture was used.</p>

<h3 id="custom-data-loader">Custom data loader</h3>

<p>In order to pass generated dataset to the Pytorch model a custom data loader needs to be implemented. Unfortunately there is a great <a href="https://medium.com/fullstackai/how-to-train-an-object-detector-with-your-own-coco-dataset-in-pytorch-319e7090da5">tutorial</a> how to implement the data loader for a custom COCO dataset. Also the Pytorch has a good boilerplate for detection training scripts that can be found in their <a href="https://github.com/pytorch/vision/tree/master/references/detection">github repo</a>. Combining these two great sources allowed to write the training script more easily.</p>

<h3 id="augmentations">Augmentations</h3>

<p>Detection task needs a little more work than classification task when it comes to image augmentation as the original bounding boxes on images also need to shift and follow the augmented image corresponding sections, so that they also match the cell after augmentation. As the annotation was considered on the image if the annotation center point was on the selected tile, then the bounding boxes shape also need to be limited by the border of the image.</p>

<p><img src="/histpath/images/histpath/limit_bboxes.png" alt="" title="Cutting excessive bounding boxes" /></p>

<p>In order for the model to generalize and achieve better performance image augmentations were used in training process. <a href="http://albumentations.ai/">Albumentations</a> library was used in order to distort the original images. The training images transforms consisted of <a href="https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L288">Flipping</a>, <a href="https://github.com/albumentations-team/albumentations/blob/a3f40853b33de19f8c64f5d977190a6cdd0c786c/albumentations/augmentations/transforms.py#L1635-L1677">RandomBrightnessContrast</a>, <a href="https://github.com/albumentations-team/albumentations/blob/a3f40853b33de19f8c64f5d977190a6cdd0c786c/albumentations/augmentations/transforms.py#L1414-L1457">HueSaturationValue</a> augmentations and <a href="https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L1993">CLAHE</a> or <a href="https://github.com/albumentations-team/albumentations/blob/master/albumentations/augmentations/transforms.py#L2466">FancyPCA</a> augmentations.</p>

<p><img src="/histpath/images/histpath/augmentations.png" alt="" title="Augmentation of training images" /></p>

<h3 id="training">Training</h3>

<p>Model was trained on the Tartu University <a href="https://hpc.ut.ee/en/home/">HPC</a> (High Performance Computing Center) using NVIDIA Tesla V100 GPU.</p>

<h3 id="using-images-from-different-levels">Using images from different levels</h3>

<p>After implementing the first model with only level 1 (the layer with largest dimensions) images it seemed that the model might improve by seeing cells from different height as human does when annotating different cell types. The level 1 image might provide higher details on how the exact cell looks and how to distinguish between different cells, but the higher level images might provide overview regarding cell positioning, detection etc.</p>

<p><img src="/histpath/images/histpath/scaling_layers.png" alt="" title="Images from different levels" /></p>

<p>Taking images from different levels meant also that the bounding boxes of level 1 annotations needed to be scaled accordingly. As each level were scaled to cover twice the dimension of previous level, then using images above level 3 seemed to be not effective as the annotations got smaller and smaller.</p>

<h2 id="validation">Validation</h2>

<h3 id="eliminating-predictions">Eliminating predictions</h3>

<p>Non-Max Suppression is a technique to filter the predictions of object detections. The idea of the Non-Max Suppression is to keep the highest scoring bounding box for a single detection. The IOU (intersection over union) of two bounding boxes is calculated to determine the boxes that overlap and lowest scoring boxes are dropped. This can reduce the predictions noise quite a lot as can be seen in the following image.</p>

<p><img src="/histpath/images/histpath/supression_vs_none.png" alt="" title="Using non-max suppression" /></p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TODO:
Mainly, I have to improve model performance.
Current issues:
- model detects cells very well but can't distinguish between different types
- The slides are not fully annotated and only some cells on the slides are.
  This creates a situation where cells are found, but as all of them are not annotated
  they are considered falsy. This allows model to find cells but not distinguish
  between different types.
</code></pre></div></div>

<h2 id="conclusion">Conclusion</h2>

<p>There is lot of work before training the neural network model. Data comes in all sorts of different formats and the preprocessing takes most of the time. Also the job isn’t finished when you have reached the model training phase, there is always room for improvement. Otherwise, lots of new knowledge with WSI, Pytorch, Python, FastPages, using HPC Cluster etc.</p>

<p>Source Code available in:
<a href="https://github.com/sinukaarel/histpath-data">GitHub</a>
<img src="/histpath/images/histpath/qrcode.png" alt="" /></p>

<p>Some Good Resources:</p>

<ul>
  <li>Downscaling whole-slide images in Python <a href="https://developer.ibm.com/technologies/data-science/articles/an-automatic-method-to-identify-tissues-from-big-whole-slide-images-pt1/">link</a></li>
  <li>Use tiling and generate smaller images where the sample occupying atleast 90% of the area <a href="https://web.stanford.edu/group/rubinlab/pubs/2243353.pdf">link</a></li>
  <li>Deephistopath <a href="https://github.com/CODAIT/deep-histopath">link</a></li>
</ul>

  </div><a class="u-url" href="/histpath/histopathology/2020/01/21/histpath.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/histpath/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/histpath/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/histpath/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>An easy to use blogging platform with support for Jupyter Notebooks.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/fastai" title="fastai"><svg class="svg-icon grey"><use xlink:href="/histpath/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/fastdotai" title="fastdotai"><svg class="svg-icon grey"><use xlink:href="/histpath/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
